{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data,datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### torchtext.data\n",
    "\n",
    "    - data.dataset\n",
    "    - data.Iterator.split\n",
    "    \n",
    "There are two crucial concepts (in high level concept) in torchtext : dataset and Iterator\n",
    "\n",
    "`dataset` object hold field(which I explain later), examples, and several other settings.  \n",
    "`Iterator` object literally iterate over batches of the whole dataset. It manages batches efficiently with many options including batch_size, shuffle, sort_key etc.\n",
    "We can construct dataset objects by using examples and fields and in case where we have data format of tsv, csv, json, by using `TabularDataset` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field object\n",
    "\n",
    "    - data.Field\n",
    "    \n",
    "Field object holds vocabs and word embeddings of given source. You can easily manage and make stats of vocabs and lables with `Field` object. One can set preprocessing pipeline of his/her own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_field.preprocessing = lambda x:x ## Doing nothing is default preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### create text and label fields : they hold vocabs and labels respectively\n",
    "### tokenize option specifies which tokenizer to use. if 'spacy', it use spacy english tokenizer. \n",
    "### If not specified, it uses default str.split() method.\n",
    "### Set sequential option true if you want seq to seq modeling.\n",
    "### tensor_type determines types of batch tensors you get \n",
    "\n",
    "text_field = data.Field(lower=True, tokenize='spacy',tensor_type=torch.LongTensor)\n",
    "label_field = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### create dataset object and iterator object\n",
    "### fields designate the meaning of each column. Here, first column stands for 'text' and second column means 'label'\n",
    "\n",
    "pr_data = data.TabularDataset(path='polarity.tsv',format='tsv',fields=[('text',text_field),('label',label_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### split pr_data into train and test sets :\n",
    "### unfortunately, there is no method for data spliting in torchtext, so we manually construct train and test dataset objects\n",
    "### we can construct dataset object if we have 'example' and 'fields'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "examples =  pr_data.examples\n",
    "np.random.shuffle(examples)\n",
    "\n",
    "# train : test = 8 : 2\n",
    "train_ex =examples[:int(len(examples)*0.8)]\n",
    "test_ex =examples[int(len(examples)*0.8):]\n",
    "\n",
    "train_data = data.Dataset(examples=train_ex,fields={'text':text_field,'label':label_field})\n",
    "test_data = data.Dataset(examples=test_ex,fields={'text':text_field,'label':label_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### build vocabs of each fields : create vocab object which counts and manages vocabularies in data \n",
    "### In case of label_field, vocabularies would be 'True' and 'False' or '1' and '0' sort of things\n",
    "### you can download pre-trained embeddings if you specify vector='glove.6B.100d' option (many other embeddings are supported)\n",
    "\n",
    "text_field.build_vocab(train_data,test_data) # vector='glove.6B.100d'\n",
    "label_field.build_vocab(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Iterator.splits` method receives `Dataset` object and returns corresponding `Iterator` object. Helpful if multiple Dataset objects need converting. Be aware that in case of one `Dataset` object being converted, returned object is also wrapped in tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter,test_iter = data.Iterator.splits(datasets=(train_data,test_data),\n",
    "                                        batch_sizes = (20,20),\n",
    "                                        repeat=False,  # if you don't specify this, your iterator will not stop yielding batches.\n",
    "                                        sort_key = lambda ex:len(ex.text)) # torchtext supports dynamic padding. Not to pad overly,\n",
    "                                                                            # Iterator sorts texts with key of text length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train simple lstm model\n",
    "\n",
    "We build simple lstm model for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,vocab_size):    \n",
    "        super(LSTM,self).__init__()\n",
    "        self.lstm = nn.LSTM(100, 200, 2, batch_first= True)\n",
    "        self.fc = nn.Linear(200, 2)\n",
    "        self.embedding = nn.Embedding(vocab_size,100)\n",
    "    def forward(self, x, h_0, c_0):\n",
    "        x = self.embedding(x)\n",
    "        x, (_, _) = self.lstm(x,(h_0,c_0))\n",
    "        x = x[:,x.size(1)-1,:].squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | batch 73 | acc : 95.00 | loss : 0.10128"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-41fc81d60673>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh_0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient has to be a Tensor, Variable or None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### model train\n",
    "\n",
    "num_epoches = 50\n",
    "vocab_size = len(text_field.vocab.itos)\n",
    "\n",
    "model = LSTM(vocab_size)\n",
    "model.cuda()\n",
    "model.train()\n",
    "\n",
    "hidden state, cell state initiation\n",
    "h_0,c_0 = Variable(torch.zeros(2,20,200)).cuda(), Variable(torch.zeros(2,20,200)).cuda()\n",
    "\n",
    "# setting parameter optimizer\n",
    "optimizer= torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(1,num_epoches+1):\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        feature,target = batch.text, batch.label\n",
    "        feature.data.t_(), target.data.sub_(1)  # batch first and index align : label_field contains <unk>, '0', '1' \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(feature,h_0,c_0)\n",
    "        loss = F.cross_entropy(out,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = (torch.max(out.data,1)[1] == target.data).sum()\n",
    "        total = target.data.size(0)\n",
    "        accuracy = correct/total * 100.0\n",
    "        \n",
    "        sys.stdout.write('\\repoch %d | batch %d | acc : %.2f | loss : %.4f'%(epoch,idx+1, accuracy, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate the model\n",
    "\n",
    "we evaluate the model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmis-peter\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "validation | acc : 49.75 | loss : 1.5857"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "avg_loss = .0\n",
    "\n",
    "model.eval()\n",
    "for batch in test_iter:\n",
    "    feature,target = batch.text, batch.label\n",
    "    feature.data.t_(), target.data.sub_(1)\n",
    "    feature.volatile, target.volatile = True, True\n",
    "    \n",
    "    out = model(feature,h_0,c_0)\n",
    "    feature.volatile, target.volatile = False, False\n",
    "    \n",
    "    avg_loss += F.cross_entropy(out,target).data[0]\n",
    "    correct += (torch.max(out.data,1)[1] == target.data).sum()\n",
    "    total += target.data.size(0)\n",
    "    \n",
    "avg_loss /= len(test_iter)\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "sys.stdout.write('\\rvalidation | acc : %.2f | loss : %.4f'%(accuracy, avg_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
